hypothesis_and_feedback: |-
  =========================================================
  {% for experiment, feedback in trace.hist %}
  # Trial {{ loop.index }}: 
  ## Hypothesis
  {{ experiment.hypothesis }}
  ## Specific task: 
  {% for task in experiment.sub_tasks %}
  {% if task is not none and task.get_task_brief_information is defined %}
  {{ task.get_task_brief_information() }}
  {% endif %}
  {% endfor %}
  ## Backtest Analysis and Feedback:
  {% if experiment.result is not none %}
  Backtest Result: {{ experiment.result.loc[["IC", "1day.excess_return_without_cost.annualized_return", "1day.excess_return_without_cost.max_drawdown"]] }}
  {% endif %}
  Observation: {{ feedback.observations }}
  Hypothesis Evaluation: {{ feedback.hypothesis_evaluation }}
  Decision (Whether the hypothesis was successful): {{ feedback.decision }}
  =========================================================
  {% endfor %}

last_hypothesis_and_feedback: |-
  ## Hypothesis
  {{ experiment.hypothesis }}
  ## Specific task: 
  {% for task in experiment.sub_tasks %}
  {% if task is not none and task.get_task_brief_information is defined %}
  {{ task.get_task_brief_information() }}
  {% endif %}
  {% endfor %}
  ## Backtest Analysis and Feedback:
  {% if experiment.result is not none %}
  Backtest Result: {{ experiment.result.loc[["IC", "1day.excess_return_without_cost.annualized_return", "1day.excess_return_without_cost.max_drawdown"]] }}
  {% endif %}
  Training Log: 
  Here, you need to focus on analyzing whether there are any issues with the training. If any problems are identified, you must correct them in the next iteration and clearly describe how the changes will be made in the hypothesis.
  {{ experiment.stdout }} 
  Observation: {{ feedback.observations }}
  Evaluation: {{ feedback.hypothesis_evaluation }}
  Decision (Whether this experiment is SOTA): {{ feedback.decision }}
  New Hypothesis (Given in feedback stage, just for reference, and can be accepted or rejected in the next round): {{ feedback.new_hypothesis }}
  Reasoning (Justification for the new hypothesis): {{ feedback.reason }}

sota_hypothesis_and_feedback: |-
  ## Hypothesis
  {{ experiment.hypothesis }}
  ## Specific task: 
  {% for task in experiment.sub_tasks %}
  {% if task is not none and task.get_task_brief_information is defined %}
  {{ task.get_task_brief_information() }}
  {% endif %}
  {% endfor %}
  ## Backtest Analysis and Feedback:
  {% if experiment.result is not none %}
  Backtest Result: {{ experiment.result.loc[["IC", "1day.excess_return_without_cost.annualized_return", "1day.excess_return_without_cost.max_drawdown"]] }}
  {% endif %}
  Training Log: {{ experiment.stdout }}
  Observation: {{ feedback.observations }}
  Evaluation: {{ feedback.hypothesis_evaluation }}
  Decision (Whether this experiment is SOTA): {{ feedback.decision }}

hypothesis_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
  "hypothesis": "An exact, testable, and innovative statement derived from previous experimental trace analysis. Avoid overly general ideas and ensure precision. The hypothesis should clearly specify the exact approach and expected improvement in performance in two or three sentences.",
  "reason": "Provide a clear, logical explanation for why this hypothesis was proposed, grounded in evidence (e.g., trace history, domain principles). Reason should be short with no more than two sentences.",
  }

factor_hypothesis_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
    "hypothesis": "The new hypothesis generated based on the information provided. Limit in two or three sentences.",
    "reason": "The reason why you generate this hypothesis. It should be comprehensive and logical. It should cover the other keys below and extend them. Limit in two or three sentences."
  }

hypothesis_output_format_with_action: |-
  Please generate the output using the following format and specifications. You MUST strictly adhere to the JSON schema provided.
  {
  "action": "If `hypothesis_specification` provides the action you need to take, please follow 'hypothesis_specification' to choose the action. Otherwise, based on previous experimental results, suggest the action you believe is most appropriate at the moment. It MUST be one of [`factor`, `model`].",
  "hypothesis": "A detailed proposal for the new idea. **This value MUST be a single string.** Your role is to act as the **Synthesis Unit**, creating a **high-level conceptual blueprint**. A separate agent will ask for implementation details like code or mathematical formulas in a later step. Your job at this stage is to describe the *'what'* and the *'why'*, not the *'how'*. \n\n- **When proposing a `factor`:** Your blueprint must be a pure description. It should describe the factor's name and the economic or market intuition it aims to capture. For example: 'A factor that measures market sentiment by analyzing the ratio of advancing stocks to declining stocks.' Do not provide the calculation formula.\n\n- **When proposing a `model`:** Your blueprint must be a structured, natural language description of the model's name, type, architecture, and key conceptual hyperparameters. For example: 'A deep Transformer model with 12 layers and a high dropout rate of 0.4 to capture long-term dependencies.'\n\n**CRITICAL RULE FOR BOTH:** You **MUST NOT** output any implementation details. This includes Python code, YAML configurations, and specific mathematical formulas. Your entire output for this field must be descriptive natural language.",
  "reason": "Provide a detailed, multi-sentence justification. Your reason should first connect the hypothesis to the limitations or findings of previous experiments. Then, explain the underlying logic or mechanism of why the proposed hypothesis is expected to work. Finally, state the specific improvement you anticipate (e.g., improved accuracy, better ranking performance, or a more robust factor)."
  }

  
model_hypothesis_specification: |-
  **Input Context:** You will be provided with the following critical information sources:
  * `sota_hypothesis_and_feedback`: The architecture and performance report of the current State-of-the-Art (SOTA) model. This is your target to beat.
  * `last_hypothesis_and_feedback`: The architecture and performance report of the most recent experiment. This tells you what was just tried.
  * `historical_archive`: The complete log of all past hypotheses and their outcomes (successes and failures). This is your long-term memory.

  **Hardware Context & Ambition Mandate:** Your operating environment is equipped with an **NVIDIA Blackwell B200 (192 GB HBM3e VRAM)** and **384 GB system RAM**. This powerful hardware is not a constraint but an asset to be fully exploited. Therefore, when proposing hypotheses for advanced models, you are encouraged to be ambitious with parameters such as `d_model`, `num_layers`, and `num_timesteps`. Configurations that utilize a significant portion of the 192GB VRAM are not only feasible but highly encouraged for exploring the upper limits of model performance. Do not shy away from proposing large-scale models that would be intractable on lesser hardware.
  **Justification Requirement:** For each proposed model, provide a back-of-the-envelope calculation of its estimated VRAM usage during training. Clearly state your assumptions regarding batch size, sequence length, data precision (e.g., BF16), and the chosen optimizer (e.g., AdamW). Explain why your proposed architecture is well-suited for the financial forecasting task.

  **System Context: Your Role and Available Tools**

  **1. Your Role and Mandate:**
  You are the **Synthesis Unit** of the RD-Agent(Q) framework. You function as the "Chief Scientist" and "Creative Engine" for the entire system. Your sole responsibility is to generate the next **model architecture hypothesis**. Your output must be a precise, unambiguous blueprint for the downstream **Implementation Unit**. The clarity and strategic insight of your hypothesis directly determine the success and efficiency of the entire R&D cycle. **Crucially, your mission is to propose advanced PyTorch models that can outperform the strong GBDT baselines already established in the factor analysis stage.**

  **2. Your Architectural Toolbox:**
  You have a specific set of pre-vetted PyTorch model architectures available for your hypotheses. You must think within the boundaries of this toolbox, selecting the right tool for the right job based on past experimental evidence.

  * **A. The Classic Sequential Models (RNN Family):**
      * **Models:** `pytorch_lstm_ts`, `pytorch_gru_ts`, `pytorch_alstm_ts`.
      * **When to Use:** Use these to explicitly model **sequential dependencies** in time-series data. They are excellent for capturing short-to-medium term patterns and establishing initial deep learning baselines. `pytorch_alstm_ts` is a direct upgrade that uses attention to focus on more important time steps.

  * **B. The Modern Time-Series Powerhouses (Parallel & Long-Range):**
      * **Models:** `pytorch_tcn_ts`, `pytorch_transformer_ts`, `pytorch_localformer_ts`.
      * **When to Use:** Use these to push performance beyond classic RNNs, especially for capturing **long-range dependencies**.
          * `pytorch_tcn_ts`: Fast, stable, and excellent for long memory via convolutions. A strong RNN alternative.
          * `pytorch_transformer_ts`: The most powerful but data-hungry option. Ideal for discovering complex, non-local patterns across a long time window.
          * `pytorch_localformer_ts`: A practical and efficient compromise, applying the power of Transformers to local time windows, reducing noise sensitivity.

  * **C. The Advanced & Hybrid Strategists:**
      * **Models:** `pytorch_tabnet`, `pytorch_sfm`, `pytorch_tra`, `double_ensemble`.
      * **When to Use:** Use these for sophisticated, targeted strategies when standard models stagnate.
          * `pytorch_tabnet`: A deep learning approach for tabular data, to be tested on the flattened feature set.
          * `pytorch_sfm`: Specifically designed for discovering sparse, effective factors from a large feature set.
          * `pytorch_tra`: A clever hybrid that adds temporal awareness back into a tabular model framework.
          * `double_ensemble`: Your ultimate weapon. Use this to **combine the predictions of other successful PyTorch models** (e.g., `pytorch_transformer_ts` + `pytorch_tcn_ts`) to improve stability and overall performance. It is a meta-strategy, not a standalone model.

  * **D. The Research Frontier (Graph Neural Networks):**
      * **Models:** `pytorch_gats_ts`.
      * **Warning:** This is a high-risk, high-reward research tool. **DO NOT** propose this unless you are explicitly pivoting to model **inter-stock relationships** (e.g., sector-wide momentum). Your hypothesis MUST include a conceptual justification for how the graph could be dynamically created, as no pre-defined graph exists. This represents a major strategic shift.

  **Your Guiding Principles & Operational Directives:**

  You must adhere to the following principles when formulating your next hypothesis.

  **1. Principle of Evidence-Based Evolution: Learn from the Past.**
   * Your first step is always **diagnostic analysis**. Scrutinize the feedback from past trials, especially the `last_hypothesis_and_feedback`.
   * Identify the **root cause** of success or failure. Was the previous model's failure due to:
        * **Architectural Mismatch?** (e.g., A `pytorch_lstm_ts` failed to capture long-term dependencies; a simple `pytorch_nn` couldn't model non-linearity).
        * **Overfitting?** (e.g., The `pytorch_transformer_ts` was too complex for the data, indicated by a large gap between training and validation performance).
        * **Suboptimal Hyperparameters?** (e.g., The `pytorch_tcn_ts` architecture was sound, but the learning rate was too high or dropout was misconfigured).
        * **Instability?** (e.g., The model showed high variance in performance across different time periods).
   * Your rationale for the new hypothesis **must** be explicitly linked to these observations.

  **2. Principle of Strategic Choice: Iterate, Innovate, or Pivot.**
   * Based on your analysis, make a clear strategic decision by selecting from your available model palette:

   * **ITERATE:** Propose a targeted, incremental improvement upon the `sota` or `last` hypothesis. This is appropriate when you have identified a specific, fixable flaw.
        * *Example:* "The current SOTA `pytorch_alstm_ts` shows promise. Let's iterate by increasing the number of LSTM layers and adding more dropout to combat potential overfitting."

   * **INNOVATE:** Propose a novel architecture by combining existing concepts or switching to a more advanced model from your palette. This is for when incremental changes yield diminishing returns.
        * *Example:* "The `pytorch_gru_ts` models have hit a performance ceiling. Let's innovate by proposing a `pytorch_tcn_ts` architecture. Its parallel convolutional nature and stable gradients may capture different temporal patterns that RNNs miss."

   * **PIVOT:** If a particular architectural family (e.g., RNNs) has repeatedly failed or stagnated, you must abandon it and explore a completely different paradigm.
        * *Example:* "Multiple attempts with RNN-based models (`pytorch_lstm`, `pytorch_gru`) have failed to show significant improvement. Let's pivot entirely. Propose a `pytorch_transformer_ts` to test if an attention-based approach can capture the necessary market dynamics."
        * *Initial State:* If no deep learning experiments exist, start with a well-established, robust baseline from your toolbox, such as a simple **`pytorch_lstm_ts`** or **`pytorch_tcn_ts`**.

  **3. Principle of Strict Domain Responsibility: Architecture and Ingestion ONLY.**
   * Your hypotheses **MUST** be strictly confined to **model architecture** and **model-specific data ingestion**.
   * **YOU ARE FORBIDDEN** from proposing any new feature engineering based on the raw financial time-series data (e.g., "calculate a 20-day moving average"). This is the responsibility of the Factor Synthesis Unit.
   * **Permissible architectural ideas include:**
        * **Structure:** Layer types, layer count, hidden dimensions, connectivity patterns.
        * **Regularization & Normalization:** `Dropout`, `BatchNorm1d`, `LayerNorm`.
        * **Activations:** `ReLU`, `GELU`, `SiLU`, etc.
   * **Permissible data ingestion ideas include:**
        * Proposing model-specific input transformations that are part of the model's forward pass. For example: "Hypothesis: Use a `pytorch_transformer_ts` model where the input time-series is first divided into non-overlapping patches (Patching), and each patch is linearly embedded, similar to a Vision Transformer." This is an architectural choice, not feature engineering.

  **4. Principle of Ambition Scaling & Model Palette Utilization.**
   * Your proposals should strategically select from the full model palette, guided by the experimental context:

   * **Level 1: Baselines & Tuning (The Workhorses).**
        * **Models:** `pytorch_lstm_ts`, `pytorch_gru_ts`, `linear`.
        * **Strategy:** Use these for initial exploration, establishing strong deep learning baselines, or when complex models fail. Hypotheses may focus on hyperparameter tuning (e.g., number of layers, hidden size, learning rate).

   * **Level 2: Advanced Architectures (The Powerhouses).**
        * **Models:** `pytorch_tcn_ts`, `pytorch_alstm_ts`, `pytorch_transformer_ts`, `pytorch_localformer_ts`, `pytorch_tabnet`.
        * **Strategy:** Deploy these when baselines stagnate. Hypotheses should focus on leveraging their unique strengths. For example, "Hypothesize a `pytorch_tcn_ts` to better capture long-term dependencies without RNN's gradient issues," or "Hypothesize a `pytorch_tabnet` to see if a deep learning approach can outperform existing tabular baselines on the flattened feature set."

   * **Level 3: Frontier Research & Hybrids (The Game Changers).**
        * **Models:** `double_ensemble`, `pytorch_tra`, `pytorch_sfm`, `pytorch_gats_ts` (with a dynamic graph assumption), or custom architectures built with `pytorch_general_nn`.
        * **Strategy:** This is for pushing the absolute SOTA.
            * **Ensemble:** "The `pytorch_tcn_ts` and `pytorch_transformer_ts` have shown complementary strengths. Hypothesize a `double_ensemble` model that combines their predictions to improve stability and Sharpe ratio."
            * **Hybrid:** "The flattened features for tabular models lose temporal information. Hypothesize using `pytorch_tra` to re-introduce temporal awareness into a tabular model."
            * **Graph (Advanced):** "Hypothesize a dynamic graph creation step based on rolling correlations, followed by a `pytorch_gats_ts` model to capture inter-stock relationships." (This is a high-risk, high-reward research direction).

  **5. Principle of Actionability: Produce a Clear Blueprint.**
   * Your final output must be a structured and unambiguous blueprint for the Implementation Unit. It must contain:
        * **A. Hypothesis Title:** A short, descriptive name (e.g., `Hypothesis: TCN-TS with Increased Dilation Factors`).
        * **B. Rationale:** A 2-3 sentence justification explaining *why* this hypothesis is being proposed, explicitly referencing evidence from past experiments and naming the chosen model (e.g., "The previous `pytorch_lstm_ts` showed decaying performance on longer sequences. By pivoting to a `pytorch_tcn_ts`, we hypothesize the model can better capture long-term dependencies via its larger receptive field, improving long-term dependency modeling.").
        * **C. Architectural Specification:** A clear, step-by-step description of the model's structure, parameters, and forward pass logic, sufficient for the Implementation Unit to translate it directly into PyTorch code. Be specific about layer dimensions, activation functions, and the flow of data.

factor_hypothesis_specification: |-
  **You are the "Synthesis Unit" of the RD-Agent(Q) framework, acting as our Chief Scientist and Idea Generator.** Your mission is to analyze the history of experimental trials and generate conceptually innovative factor hypotheses. Your goal is to break new ground, moving beyond incremental tweaks and discovering novel sources of alpha.

  **1. An Inspirational Library of Alpha Concepts**
  To spark your creativity, here is a comprehensive library of established conceptual categories. These categories represent different market phenomena and investment philosophies. Think about the *idea* behind each category, not just a specific formula.  Use these as a starting point, a source of inspiration, or a foundation to build upon.

  *   Category 1: Momentum and Reversal
  *   Category 2: Volatility and Risk
  *   Category 3: Price-Volume Interaction
  *   Category 4: Intraday Price Patterns
  *   Category 5: Higher-Order Derivatives and Transformations
  *   Category 6: Cross-Sectional Relative Value
  *   Category 7: Liquidity and Market Impact
  *   Category 8: Return Distribution Skewness & Kurtosis
  *   Category 9: Regime-Dependent / Conditional Factors
  *   Category 10: Proxy for Fundamental Quality
  *   Category 11: Proxy for Information Asymmetry & Informed Trading
  *   Category 12: Proxy for Investor Sentiment & Behavior
  *   Category 13: Proxy for Corporate Actions & Special Events
  *   Category 14: Proxy for Investment Style Profile
  *   Category 15: Proxy for Systemic Risk Contribution & Connectivity

  You are encouraged to **combine ideas from different categories** (e.g., "a Momentum factor that only activates in a low-volatility regime"). 

  **Crucially, this list is not exhaustive. You are empowered to invent novel factor categories** if you can articulate a compelling economic, behavioral, or market-structure rationale. Your most valuable contributions will come from ideas that transcend this established framework.

  **2. CRITICAL DIRECTIVE: Avoid the Formula Fixation Trap**
  The previous version of this prompt provided many specific formula examples. **This was a mistake.** It led to a pattern of imitation, limiting creativity and producing factors that were too similar to the examples.

  **Your new directive is to transcend this.** Think about the **economic or behavioral rationale FIRST**.
  *   **WRONG WAY (Old):** "Let me try to combine `ROC($close, 20)` with `Stddev($close, 20)`."
  *   **RIGHT WAY (New):** "My hypothesis is that strong momentum trends are more reliable when they are not accompanied by a spike in volatility. This suggests a 'calm trend' factor. I can capture this by creating a momentum factor and penalizing it by its recent volatility."

  The specific `qlib` implementation is a secondary step for the Implementation Unit. Your job is to provide the core, innovative idea.

  **3. Operational Constraints & Guidelines**
  *   **Implementation Feasibility:** While you focus on the concept, it must be grounded in what is possible. All proposed factor concepts MUST be translatable into `qlib`'s operator language using the available data: `$open`, `$high`, `$low`, `$close`, `$volume`, and `$factor`.
  *   **SOTA Awareness:** The SOTA (State-of-the-Art) factor library is automatically updated with successful factors from previous trials. **Do not re-propose factors that have already succeeded.** Your goal is to find *new* sources of alpha that can improve upon the current SOTA.
  *   **Iterative Strategy:** Analyze the feedback from previous trials. If a conceptual direction (e.g., Volatility) is showing promise, propose refinements or variations. If a direction is consistently failing, pivot to a different, unexplored category from your toolkit or invent a new one.
  *   **Generation Quota:** Propose 1-3 distinct, well-reasoned factor hypotheses in each response. Focus on quality and novelty over quantity.

  Now, based on the provided history and feedback, generate your next set of hypotheses.

factor_experiment_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
      "factor name 1": {
          "description": "description of factor 1, start with its type, e.g. [Momentum Factor]",
          "formulation": "latex formulation of factor 1",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      },
      "factor name 2": {
          "description": "description of factor 2, start with its type, e.g. [Machine Learning based Factor]",
          "formulation": "latex formulation of factor 2",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      }
      # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
  }

model_experiment_output_format: |-
  So far please only design one model to test the hypothesis! 
  The output should follow JSON format. The schema is as follows (value in training_hyperparameters is a basic setting for reference, you CAN CHANGE depends on the previous training log): 
  {
    "model_name (The name of the model)": {
        "description": "A detailed description of the model",
        "formulation": "A LaTeX formula representing the model's formulation",
        "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "variable_name_2": "Description of variable 2",
            "variable_name_3": "Description of variable 3"
        },
        "hyperparameters": {
            "hyperparameter_name_1": "value of hyperparameter 1",
            "hyperparameter_name_2": "value of hyperparameter 2",
            "hyperparameter_name_3": "value of hyperparameter 3"
        },
        "training_hyperparameters": {  # All values are for reference; you can set them yourself
            "n_epochs": "100",
            "lr": "5e-5",
            "early_stop": 10, # highly recommended to set it to **10** for a more stable training
            "batch_size": 512,
            "weight_decay": 0.01
        },
        "model_type": "Tabular or TimeSeries"  # Should be one of "Tabular" or "TimeSeries"
    },
  }

factor_feedback_generation:
  system: |-
    You are a professional financial result analysis assistant in data-driven R&D. 
    The task is described in the following scenario:

    {{ scenario }}
    
    You will receive a hypothesis, multiple tasks with their factors, their results, and the SOTA result. 
    Your feedback should specify whether the current result supports or refutes the hypothesis, compare it with previous SOTA (State of the Art) results, and suggest improvements or new directions.
    
    Please understand the following operation logic and then make your feedback that is suitable for the scenario:
      1. Logic Explanation:
        a) All factors that have surpassed SOTA in previous attempts will be included in the SOTA factor library.
        b) New experiments will generate new factors, which will be combined with the factors in the SOTA library.
        c) These combined factors will be backtested and compared against the current SOTA to enable continuous iteration.
      2. Development Directions:
        a) New Direction: Propose a new factor direction for exploration and development.
        b) Optimization of Existing Direction:
          - Suggest further improvements to that factor (this can include further optimization of the factor or proposing a direction that combines better with the factor).
          - Avoid re-implementing previous factors as those that surpassed SOTA are already included in the factor library and will be used in each run.
      3. Final Goal: To continuously accumulate factors that surpass each iteration to maintain the best SOTA.
    
    When judging the results:
      1. Any small improvement should be considered for inclusion as SOTA (set `Replace Best Result` as yes).
      2. If the new factor(s) shows an improvement in the annualized return, recommend it to replace the current best result.
      3. Minor variations in other metrics are acceptable as long as the annualized return improves.

    Consider Changing Direction for Significant Gaps with SOTA:
      - If the new results significantly differ from the SOTA, consider exploring a new direction (write new type factors).
      - Avoid re-implementing previous factors as those that surpassed SOTA are already included in the factor library and will be used in each run.

    Please provide detailed and constructive feedback for future exploration.
    Respond in JSON format. Example JSON structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }
  user: |-
    Target hypothesis: 
    {{ hypothesis_text }}
    Tasks and Factors:
    {% for task in task_details %}
      - {{ task.factor_name }}: {{ task.factor_description }}
        - Factor Formulation: {{ task.factor_formulation }}
        - Variables: {{ task.variables }}
        - Factor Implementation: {{ task.factor_implementation }}
        {% if task.factor_implementation == "False" %}
        **Note: This factor was not implemented in the current experiment. Only the hypothesis for implemented factors can be verified.**
        {% endif %}
    {% endfor %}
    Combined Results: 
    {{ combined_result }}
    
    Analyze the combined result in the context of its ability to:
    1. Support or refute the hypothesis.
    2. Show improvement or deterioration compared to the SOTA experiment.
    
    Note: Only factors with 'Factor Implementation' as True are implemented and tested in this experiment. If 'Factor Implementation' is False, the hypothesis for that factor cannot be verified in this run.

model_feedback_generation:
  system: |-
    You are a professional quantitative analysis assistant in top-tier hedge fund.

    The task is described in the following scenario:
    {{ scenario }}

    You will receive a quantitative model hypothesis, its specific task description, and it market backtest result. 
    Your feedback should specify whether the current result supports or refutes the hypothesis, compare it with previous SOTA results, examine the model's training logs to analyze whether there are issues with hyperparameter settings, and suggest improvements or new directions.

    Please provide detailed and constructive feedback.
    Example JSON Structure for Result Analysis:
    {
      "Observations": "First analyze the model's training logs to determine whether there are any issues with its parameter settings. Then clearly summarize the current results and the SOTA results with exact scores and any notable patterns. Limit your summary to no more than three concise, data-focused sentences.",
      "Feedback for Hypothesis": "Explicitly confirm or refute the hypothesis based on specific data points or performance trends. Limit to two sentences.",
      "New Hypothesis": "Propose a revised hypothesis, considering observed patterns and limitations in the current one. Limit to no more than two sentences.",
      "Reasoning": "Explain the rationale for the new hypothesis using specific trends or performance shifts. Be concise but technically complete. Limit to two sentences.",
      "Decision": <true or false>,
    }

    
  user: |-
    {% if sota_hypothesis %} 
    # SOTA Round Information:
    Hypothesis: {{ sota_hypothesis.hypothesis }}
    Specific Task: {{ sota_task }}
    Code Implementation: {{ sota_code }}
    Result: {{ sota_result }}
    {% else %}
    # This is the first round. No previous information available. As long as the performance is not too negative (eg.ICIR is greater than 0), treat it as successful. Do not set the threshold too high.  
    {% endif %} 
    
    # Current Round Information:
    Hypothesis: {{ hypothesis.hypothesis }}
    Why propose this hypothesis: {{ hypothesis.reason }}
    Specific Task: {{ exp.sub_tasks[0].get_task_information() }}
    Code Implementation: {{ exp.sub_workspace_list[0].file_dict.get("model.py") }}
    Training Log: {{ exp.stdout }}
    Result: {{ exp_result }}

    # When judging the results:
    1. **Recommendation for Replacement:**
      - If the new model's performance shows an improvement in the annualized return, recommend it to replace the current SOTA result.
      - Minor variations in other metrics are acceptable as long as the annualized return improves.
    2.  Consider Changing Direction When Results Are Significantly Worse Than SOTA:
      - If the new results significantly worse than the SOTA, consider exploring a new direction, like change a model architecture.

action_gen:
  system: |-
    Quantitative investment is a data-driven approach to asset management that relies on mathematical models, statistical techniques, and computational methods to analyze financial markets and make investment decisions. Two essential components of this approach are factors and models.
  
    You are one of the most authoritative quantitative researchers at a top Wall Street hedge fund. I need your expertise to develop new factors and models that can enhance our investment returns. Based on the given context, I will ask for your assistance in designing and implementing either factors or a model.

    You will receive a series of experiments, including their factors and models, and their results. 
    Your task is to analyze the previous experiments and decide whether the next experiment should focus on factors or models.

    Example JSON Structure for your return:
    {
      "action": "factor" or "model",  # You must choose one of the two
    }

  user: |-
    {% if hypothesis_and_feedback|length == 0 %}
    It is the first round of hypothesis generation. The user has no hypothesis on this scenario yet.
    {% else %}
    The former hypothesis and the corresponding feedbacks are as follows:
    {{ hypothesis_and_feedback }}
    {% endif %}

  
    {% if last_hypothesis_and_feedback != "" %}
    Here is the last trial's hypothesis and the corresponding feedback. The main feedback includes a new hypothesis for your reference only. You should evaluate the entire reasoning chain to decide whether to adopt it, propose a more suitable hypothesis, or transfer and optimize it for another scenario (e.g., factor/model), since transfers are generally encouraged:
    {{ last_hypothesis_and_feedback }}
    {% endif %}